{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w_GEfwfnyew",
        "outputId": "e15106aa-efe6-4059-8093-5014ef972c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/final_project\n",
            "Collecting torch==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.16.0 (from -r requirements.txt (line 2))\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting torchtext==0.16.0 (from -r requirements.txt (line 4))\n",
            "  Downloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: ipykernel==5.5.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (5.5.6)\n",
            "Collecting spacy==3.7.2 (from -r requirements.txt (line 6))\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
            "Collecting jupyter (from -r requirements.txt (line 10))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.5.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->-r requirements.txt (line 1))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0->-r requirements.txt (line 2)) (11.0.0)\n",
            "Collecting torchdata==0.7.0 (from torchtext==0.16.0->-r requirements.txt (line 4))\n",
            "  Downloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->-r requirements.txt (line 5)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->-r requirements.txt (line 5)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->-r requirements.txt (line 5)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel==5.5.6->-r requirements.txt (line 5)) (6.3.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.2->-r requirements.txt (line 6))\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy==3.7.2->-r requirements.txt (line 6))\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.7.2->-r requirements.txt (line 6))\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (2.10.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.7.2->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext==0.16.0->-r requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->-r requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->-r requirements.txt (line 10)) (7.16.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->-r requirements.txt (line 10)) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyterlab-4.3.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (5.10.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 12)) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 13)) (3.5.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook->-r requirements.txt (line 11)) (4.3.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->-r requirements.txt (line 11)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (0.10.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->-r requirements.txt (line 11)) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->-r requirements.txt (line 11)) (4.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2->-r requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2->-r requirements.txt (line 6)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2->-r requirements.txt (line 6)) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2->-r requirements.txt (line 6)) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.2->-r requirements.txt (line 6))\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->-r requirements.txt (line 11)) (21.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 10)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 10)) (3.0.13)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx~=0.28.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 10)) (0.28.0)\n",
            "INFO: pip is looking at multiple versions of jupyterlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jupyterlab (from jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyterlab-4.3.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.6-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.5-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.4-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.3-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.2-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: pip is still looking at multiple versions of jupyterlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jupyterlab-4.2.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.8-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.7-py3-none-any.whl.metadata (16 kB)\n",
            "  Downloading jupyterlab-4.1.6-py3-none-any.whl.metadata (16 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading jupyterlab-4.1.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.19.0 (from jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 10)) (2.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.28.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx~=0.28.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx~=0.28.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r requirements.txt (line 11)) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r requirements.txt (line 11)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r requirements.txt (line 11)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r requirements.txt (line 11)) (0.22.3)\n",
            "Collecting jupyter-client (from ipykernel==5.5.6->-r requirements.txt (line 5))\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (1.8.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.19.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2->-r requirements.txt (line 6)) (1.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel==5.5.6->-r requirements.txt (line 5)) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->-r requirements.txt (line 11)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 10)) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx~=0.28.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx~=0.28.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->-r requirements.txt (line 11)) (2.22)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10)) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 10))\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.1.5-py3-none-any.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, typer, triton, smart-open, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, json5, jedi, fqdn, cloudpathlib, async-lru, nvidia-cusolver-cu12, nvidia-cudnn-cu12, jupyter-server-terminals, jupyter-client, arrow, torch, isoduration, weasel, torchvision, torchdata, torchaudio, torchtext, spacy, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.0\n",
            "    Uninstalling typer-0.15.0:\n",
            "      Successfully uninstalled typer-0.15.0\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.0.5\n",
            "    Uninstalling smart-open-7.0.5:\n",
            "      Successfully uninstalled smart-open-7.0.5\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.20.0\n",
            "    Uninstalling cloudpathlib-0.20.0:\n",
            "      Successfully uninstalled cloudpathlib-0.20.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "Successfully installed arrow-1.3.0 async-lru-2.0.4 cloudpathlib-0.16.0 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.2 jupyter-server-terminals-0.5.3 jupyterlab-4.1.5 jupyterlab-server-2.27.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 overrides-7.7.0 python-json-logger-2.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 smart-open-6.4.0 spacy-3.7.2 torch-2.1.0 torchaudio-2.1.0 torchdata-0.7.0 torchtext-0.16.0 torchvision-0.16.0 triton-2.1.0 typer-0.9.4 types-python-dateutil-2.9.0.20241206 uri-template-1.3.0 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "# Mount into drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "%cd '/content/drive/MyDrive/final_project/'\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T-U0kZ0OqEw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import random\n",
        "import transformers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import Adam\n",
        "from torchtext.vocab import vocab\n",
        "import itertools\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import typing\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess data (split data)**"
      ],
      "metadata": {
        "id": "7FQAcdBJRG9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZrFkEF_VuQO",
        "outputId": "e00737e7-b5d5-49e3-e3a4-ccde38e1de2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data saved to ./data01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2107: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and test data saved as CSV files.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Read data from train and test directories and save as text files\n",
        "def process_and_save_txt_files(path_train, path_test, output_dir='./data01', chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Reads text data from train and test directories and saves it as text files for tokenizer training.\n",
        "\n",
        "    Args:\n",
        "        path_train (str): Path to the 'train' directory containing .txt files.\n",
        "        path_test (str): Path to the 'test' directory containing .txt files.\n",
        "        output_dir (str): Directory to save the text chunks.\n",
        "        chunk_size (int): Number of samples per text file chunk.\n",
        "\n",
        "    Returns:\n",
        "        list: List of all text data collected from the .txt files.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    text_data = []\n",
        "    file_count = 0\n",
        "\n",
        "    # Gather all text files from train and test directories\n",
        "    for path in [path_train, path_test]:\n",
        "        for txt_file in Path(path).rglob('*.txt'):\n",
        "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read().strip()  # Read and strip extra whitespace\n",
        "                text_data.append(content)\n",
        "\n",
        "                # Save text data in chunks of 10,000 samples\n",
        "                if len(text_data) == chunk_size:\n",
        "                    with open(f'{output_dir}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "                        fp.write('\\n'.join(text_data))\n",
        "                    text_data = []\n",
        "                    file_count += 1\n",
        "\n",
        "    # Save remaining data\n",
        "    if text_data:\n",
        "        with open(f'{output_dir}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
        "            fp.write('\\n'.join(text_data))\n",
        "\n",
        "    print(f\"Text data saved to {output_dir}\")\n",
        "\n",
        "    return text_data\n",
        "\n",
        "# Example paths for train and test directories\n",
        "path_train = 'data/final/train/ects'\n",
        "path_test = 'data/final/test/ects'\n",
        "\n",
        "\n",
        "# Save the data from train and test directories as text files\n",
        "text_data = process_and_save_txt_files(path_train, path_test)\n",
        "# print(text_data)\n",
        "# Step 2: Train WordPiece tokenizer\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files=[str(x) for x in Path('./data01').glob('**/*.txt')],\n",
        "    vocab_size=30_000,\n",
        "    min_frequency=5,\n",
        "    limit_alphabet=1000,\n",
        "    wordpieces_prefix='##',\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        ")\n",
        "\n",
        "# Save the tokenizer model\n",
        "os.makedirs('./bert-it-1', exist_ok=True)\n",
        "tokenizer.save_model('./bert-it-1', 'bert-it')\n",
        "\n",
        "# Load the tokenizer for further use\n",
        "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n",
        "\n",
        "# Step 3: Prepare train and test data for model training\n",
        "# Split the text data into train and test sets\n",
        "train_texts, test_texts = train_test_split(text_data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Save the train and test data as CSV files\n",
        "train_df = pd.DataFrame({'text': train_texts})\n",
        "test_df = pd.DataFrame({'text': test_texts})\n",
        "\n",
        "train_df.to_csv('data/train_data.csv', index=False, encoding='utf-8')\n",
        "test_df.to_csv('data/test_data.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Training and test data saved as CSV files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess data (for MLM and NSP)**"
      ],
      "metadata": {
        "id": "oNKPatkZRaw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vEggtjpyWYt"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.lines = data_pair\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # Get random sentence pair and next sentence label\n",
        "        t1, t2, is_next_label = self._get_sentences(item)\n",
        "\n",
        "        # Randomly mask some words in the sentences\n",
        "        t1_random, t1_label = self._random_word(t1)\n",
        "        t2_random, t2_label = self._random_word(t2)\n",
        "\n",
        "        # Add [CLS], [SEP] and [PAD] tokens\n",
        "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
        "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
        "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
        "\n",
        "        # Combine sentences and add padding\n",
        "        segment_label = [1] * len(t1) + [2] * len(t2)\n",
        "        bert_input = t1 + t2\n",
        "        bert_label = t1_label + t2_label\n",
        "        padding = [self.tokenizer.vocab['[PAD]']] * (self.seq_len - len(bert_input))\n",
        "\n",
        "        bert_input.extend(padding)\n",
        "        bert_label.extend(padding)\n",
        "        segment_label = segment_label[:self.seq_len] + [0] * (self.seq_len - len(segment_label))\n",
        "\n",
        "        # Return as tensor\n",
        "        return {\n",
        "            \"bert_input\": torch.tensor(bert_input),\n",
        "            \"bert_label\": torch.tensor(bert_label),\n",
        "            \"segment_label\": torch.tensor(segment_label),\n",
        "            \"is_next\": torch.tensor(is_next_label)\n",
        "        }\n",
        "\n",
        "    def _random_word(self, sentence):\n",
        "        tokens = sentence.split()\n",
        "        output, output_label = [], []\n",
        "\n",
        "        for token in tokens:\n",
        "            prob = random.random()\n",
        "            token_id = self.tokenizer.encode(token, add_special_tokens=False)\n",
        "\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "\n",
        "                # 80% chance change token to mask token\n",
        "                if prob < 0.8:\n",
        "                    output.extend([self.tokenizer.vocab['[MASK]'] for _ in range(len(token_id))])\n",
        "\n",
        "                # 10% chance change token to random token\n",
        "                elif prob < 0.9:\n",
        "                    output.extend([random.randrange(len(self.tokenizer.vocab)) for _ in range(len(token_id))])\n",
        "\n",
        "                # 10% chance change token to current token\n",
        "                else:\n",
        "                    output.extend(token_id)\n",
        "\n",
        "                output_label.extend(token_id)  # store the original token id for label\n",
        "\n",
        "            else:\n",
        "                output.extend(token_id)\n",
        "                output_label.extend([0] * len(token_id))  # No replacement, label as 0\n",
        "\n",
        "        # flattening\n",
        "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
        "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
        "        assert len(output) == len(output_label)\n",
        "        return output, output_label\n",
        "\n",
        "\n",
        "    def _get_sentences(self, index):\n",
        "        t1, t2 = self.lines[index][0], self.lines[index][1]\n",
        "        # 50% chance for positive or negative pair\n",
        "        if random.random() > 0.5:\n",
        "            return t1, t2, 1\n",
        "        else:\n",
        "            return t1, self._get_random_line(), 0\n",
        "\n",
        "    def _get_random_line(self):\n",
        "        return self.lines[random.randrange(len(self.lines))][1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement model (Embedding)**"
      ],
      "metadata": {
        "id": "DEcfAKn9RyEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFQanhJNREnF"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "        # Initialize positional encodings\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.requires_grad = False\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * (i + 1) / d_model)))\n",
        "\n",
        "        # Add batch dimension\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))  # Use register_buffer to save on device\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure positional embeddings are on the same device as input\n",
        "        return self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "class BERTEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Embedding includes:\n",
        "    1. Token Embedding: Standard embedding matrix for tokens.\n",
        "    2. Positional Embedding: Adds positional information via sine and cosine functions.\n",
        "    3. Segment Embedding: Adds segment information (e.g., sentence A = 1, sentence B = 2).\n",
        "    The sum of these embeddings is returned as the final output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.segment_embedding = nn.Embedding(3, embed_size, padding_idx=0)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, sequence, segment_label):\n",
        "        device = next(self.parameters()).device\n",
        "        sequence = sequence.to(device)\n",
        "        segment_label = segment_label.to(device)\n",
        "\n",
        "        token_embeds = self.token_embedding(sequence)\n",
        "        position_embeds = self.position_embedding(sequence)\n",
        "        segment_embeds = self.segment_embedding(segment_label)\n",
        "\n",
        "        embeddings = token_embeds + position_embeds + segment_embeds\n",
        "        return self.dropout(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement model (Attention mechanism)**"
      ],
      "metadata": {
        "id": "ZG2yOcUPR_S1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVU8vF_vR4fY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        query, key, value = self.query(query), self.key(key), self.value(value)\n",
        "\n",
        "        # Reshape to (batch_size, heads, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
        "\n",
        "        # Apply mask\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Attention weights and dropout\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # Apply attention weights to value\n",
        "        context = torch.matmul(weights, value)\n",
        "\n",
        "        # Reshape back to (batch_size, max_len, d_model)\n",
        "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "\n",
        "        return self.output_linear(context)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        return self.fc2(self.dropout(x))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=768, heads=12, feed_forward_hidden=768 * 4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        # Self-attention\n",
        "        attended = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        attended = self.layernorm(attended + embeddings)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_out = self.dropout(self.feed_forward(attended))\n",
        "        return self.layernorm(ff_out + attended)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implement model (MLM and NSP)**"
      ],
      "metadata": {
        "id": "2TBkAKbKSNiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIPo_PG4SZ4F"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model: Bidirectional Encoder Representations from Transformers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initializes the BERT model.\n",
        "\n",
        "        :param vocab_size: Total vocabulary size\n",
        "        :param d_model: Hidden size of the BERT model\n",
        "        :param n_layers: Number of transformer layers\n",
        "        :param heads: Number of attention heads\n",
        "        :param dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Feed-forward network hidden size is 4 times the model's hidden size\n",
        "        self.feed_forward_hidden = d_model * 4\n",
        "\n",
        "        # Embedding layer: sum of token, segment, and positional embeddings\n",
        "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=d_model)\n",
        "\n",
        "        # Multi-layer transformer blocks\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, heads, self.feed_forward_hidden, dropout) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, segment_info):\n",
        "        # Attention mask for padded tokens\n",
        "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
        "\n",
        "        # Embed the input sequence\n",
        "        x = self.embedding(x, segment_info)\n",
        "\n",
        "        # Pass through multiple transformer layers\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder(x, mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NextSentencePrediction(nn.Module):\n",
        "    \"\"\"\n",
        "    2-class classification model: predicts whether the second sentence follows the first.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        \"\"\"\n",
        "        Initializes the next sentence prediction model.\n",
        "\n",
        "        :param hidden_size: The hidden size of the BERT model output\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use only the [CLS] token (first token) for classification\n",
        "        return self.softmax(self.linear(x[:, 0]))\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts the original token for each masked token in the input sequence.\n",
        "    This is a multi-class classification problem where the number of classes is the vocabulary size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        \"\"\"\n",
        "        Initializes the masked language model.\n",
        "\n",
        "        :param hidden_size: The hidden size of the BERT model output\n",
        "        :param vocab_size: The size of the vocabulary for the classification task\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "\n",
        "class BERTLM(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT Language Model combining Next Sentence Prediction (NSP) and Masked Language Modeling (MLM).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bert: BERT, vocab_size):\n",
        "        \"\"\"\n",
        "        Initializes the BERT Language Model with NSP and MLM.\n",
        "\n",
        "        :param bert: The pre-trained BERT model\n",
        "        :param vocab_size: The size of the vocabulary for the masked language model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.next_sentence = NextSentencePrediction(self.bert.d_model)\n",
        "        self.mask_lm = MaskedLanguageModel(self.bert.d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, segment_label):\n",
        "        # Pass through the BERT model to obtain embeddings\n",
        "        x = self.bert(x, segment_label)\n",
        "\n",
        "        # Get outputs for both NSP and MLM tasks\n",
        "        return self.next_sentence(x), self.mask_lm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train BERT model (warm-up)**"
      ],
      "metadata": {
        "id": "vQxl7yu6UQP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train BERT model (use checkpoint to store best model/ weights)**"
      ],
      "metadata": {
        "id": "AwSHv9nxUKBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90mYiN3oT8yT"
      },
      "outputs": [],
      "source": [
        "class ScheduledOptim():\n",
        "    \"\"\"A simple wrapper for learning rate scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        \"\"\"\n",
        "        Initializes the learning rate scheduler.\n",
        "\n",
        "        :param optimizer: The optimizer for which learning rate is scheduled\n",
        "        :param d_model: The dimension of the model (used to initialize learning rate)\n",
        "        :param n_warmup_steps: Number of warm-up steps for learning rate scheduling\n",
        "        \"\"\"\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"\"\"Step with the optimizer and update the learning rate.\"\"\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Zero out the gradients in the optimizer.\"\"\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        \"\"\"Calculates the scaling factor for the learning rate.\"\"\"\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps\n",
        "        ])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        \"\"\"Updates the learning rate based on the current step.\"\"\"\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        # Update the learning rate for each parameter group in the optimizer\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train BERT model (use checkpoint to store best model/ weights)**"
      ],
      "metadata": {
        "id": "syHU27l_TzC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lFryv-S79yg"
      },
      "outputs": [],
      "source": [
        "class BERTTrainer:\n",
        "    def __init__(self, model, train_dataloader, test_dataloader=None, lr=1e-4, weight_decay=0.01,\n",
        "                 betas=(0.9, 0.999), warmup_steps=10000, log_freq=10, checkpoint_dir=None, log_dir=None, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = model.to(device)\n",
        "        self.train_data = train_dataloader\n",
        "        self.test_data = test_dataloader\n",
        "        self.checkpoint_dir = Path(checkpoint_dir) if checkpoint_dir else None\n",
        "        self.log_dir = Path(log_dir) if log_dir else None\n",
        "        self.writer = SummaryWriter(log_dir=str(self.log_dir)) if self.log_dir else None\n",
        "\n",
        "        # Set up Adam optimizer with hyperparameters\n",
        "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        self.optim_schedule = ScheduledOptim(self.optim, self.model.bert.d_model, warmup_steps)\n",
        "\n",
        "        # Use Negative Log Likelihood loss for masked token prediction\n",
        "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
        "        self.log_freq = log_freq\n",
        "\n",
        "        print(f\"Total Parameters: {sum(p.nelement() for p in self.model.parameters())}\")\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self._iteration(epoch, self.train_data, train=True)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self._iteration(epoch, self.test_data, train=False)\n",
        "\n",
        "    def _iteration(self, epoch, data_loader, train=True):\n",
        "        avg_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_elements = 0\n",
        "        mode = \"train\" if train else \"test\"\n",
        "\n",
        "        # Set up progress bar\n",
        "        data_iter = tqdm.tqdm(enumerate(data_loader), desc=f\"EP_{mode}:{epoch}\", total=len(data_loader),\n",
        "                              bar_format=\"{l_bar}{r_bar}\")\n",
        "\n",
        "        for i, data in data_iter:\n",
        "            # Move batch data to the specified device (GPU or CPU)\n",
        "            data = {key: value.to(self.device) for key, value in data.items()}\n",
        "\n",
        "            # Forward pass for next sentence prediction and masked language model\n",
        "            next_sent_output, mask_lm_output = self.model(data[\"bert_input\"], data[\"segment_label\"])\n",
        "\n",
        "            # Calculate losses\n",
        "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
        "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"])\n",
        "\n",
        "            # Total loss: sum of next sentence prediction and masked language model losses\n",
        "            loss = next_loss + mask_loss\n",
        "\n",
        "            # Backward pass and optimization (only in training)\n",
        "            if train:\n",
        "                self.optim_schedule.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "            # Accuracy for next sentence prediction\n",
        "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
        "            avg_loss += loss.item()\n",
        "            total_correct += correct\n",
        "            total_elements += data[\"is_next\"].nelement()\n",
        "\n",
        "            # Log progress\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"avg_loss\": avg_loss / (i + 1),\n",
        "                \"avg_acc\": total_correct / total_elements * 100,\n",
        "                \"loss\": loss.item()\n",
        "            }\n",
        "\n",
        "            if self.writer and i % self.log_freq == 0:\n",
        "                data_iter.write(str(post_fix))\n",
        "                self.writer.add_scalar(\"Loss/Train\", loss.item(), epoch * len(data_loader) + i)\n",
        "                self.writer.add_scalar(\"Accuracy/Train\", total_correct / total_elements * 100, epoch * len(data_loader) + i)\n",
        "\n",
        "        # Print summary for the epoch\n",
        "        print(f\"EP{epoch}, {mode}: avg_loss={avg_loss / len(data_iter)}, total_acc={total_correct * 100.0 / total_elements}\")\n",
        "\n",
        "        if self.checkpoint_dir and (epoch % 1 == 0):  # save checkpoint every epoch\n",
        "            self.save_checkpoint(epoch)\n",
        "\n",
        "    def save_checkpoint(self, epoch):\n",
        "        if not self.checkpoint_dir:\n",
        "            return\n",
        "\n",
        "        checkpoint_filename = f\"bert_epoch{epoch}_{datetime.utcnow().timestamp():.0f}.pt\"\n",
        "        checkpoint_path = self.checkpoint_dir / checkpoint_filename\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optim.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        print(f\"Restoring model from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(\"Model restored from checkpoint.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The training records for BERT-Large model**"
      ],
      "metadata": {
        "id": "aJIPHs2WUlRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "try:\n",
        "    BASE_DIR = Path(__file__).resolve().parent\n",
        "except NameError:\n",
        "    # For interactive environments like Jupyter\n",
        "    BASE_DIR = Path(os.getcwd())\n",
        "\n",
        "CHECKPOINT_DIR = BASE_DIR.joinpath('data/bert_checkpoints')\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.utcnow().timestamp()\n",
        "LOG_DIR = BASE_DIR.joinpath(f'data/logs/bert_experiment_{timestamp}')\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MAX_LEN = 64\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.00001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "BETAS = (0.9, 0.999)\n",
        "WARMUP_STEPS = 4000\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Preparing dataset...\")\n",
        "\n",
        "    train_data = BERTDataset(train_texts, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "    test_data = BERTDataset(test_texts, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Dataset loaded(train): {len(train_data)} samples.\")\n",
        "    print(f\"Dataset loaded(test): {len(test_data)} samples.\")\n",
        "\n",
        "    print(\"Initializing model...\")\n",
        "    bert_model = BERT(\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        d_model=1024,\n",
        "        n_layers=24,\n",
        "        heads=16,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # Wrap the BERT model with language modeling components\n",
        "    bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "    bert_lm.to(device)\n",
        "\n",
        "    # Initialize the trainer\n",
        "    bert_trainer = BERTTrainer(\n",
        "        model=bert_lm,\n",
        "        train_dataloader=train_loader,\n",
        "        test_dataloader=test_loader,\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        betas=BETAS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        log_freq=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        bert_trainer.train(epoch)\n",
        "\n",
        "        # Test\n",
        "        print(\"Evaluating on test data...\")\n",
        "        bert_trainer.test(epoch)\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        checkpoint_path = CHECKPOINT_DIR.joinpath(f\"bert_checkpoint_epoch_{epoch + 1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': bert_lm.state_dict(),\n",
        "            'optimizer_state_dict': bert_trainer.optim.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXHuBCoLVrsF",
        "outputId": "35090788-f544-4d4a-c449-a53178154511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset...\n",
            "Dataset loaded(train): 1740 samples.\n",
            "Dataset loaded(test): 436 samples.\n",
            "Initializing model...\n",
            "Total Parameters: 351476179\n",
            "Starting training...\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0: 100%|| 55/55 [01:00<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP0, train: avg_loss=8.00240068435669, total_acc=48.67816091954023\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:0: 100%|| 14/14 [00:05<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP0, test: avg_loss=6.012485810688564, total_acc=48.1651376146789\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_1.pt\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:1: 100%|| 55/55 [01:02<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP1, train: avg_loss=5.145532950488004, total_acc=50.804597701149426\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:1: 100%|| 14/14 [00:05<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP1, test: avg_loss=4.4224024670464654, total_acc=50.91743119266055\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_2.pt\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:2: 100%|| 55/55 [01:04<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP2, train: avg_loss=4.081280127438632, total_acc=47.35632183908046\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:2: 100%|| 14/14 [00:06<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP2, test: avg_loss=3.8755704164505005, total_acc=49.31192660550459\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_3.pt\n",
            "Epoch 4/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:3: 100%|| 55/55 [01:05<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP3, train: avg_loss=3.5977952350269664, total_acc=51.724137931034484\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:3: 100%|| 14/14 [00:06<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP3, test: avg_loss=3.018637844494411, total_acc=47.018348623853214\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_4.pt\n",
            "Epoch 5/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:4: 100%|| 55/55 [01:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP4, train: avg_loss=nan, total_acc=49.08045977011494\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:4: 100%|| 14/14 [00:06<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP4, test: avg_loss=2.7191762157848904, total_acc=52.293577981651374\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_5.pt\n",
            "Epoch 6/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:5: 100%|| 55/55 [01:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP5, train: avg_loss=2.7386115529320456, total_acc=51.55172413793103\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:5: 100%|| 14/14 [00:06<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP5, test: avg_loss=2.75054863521031, total_acc=47.018348623853214\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_6.pt\n",
            "Epoch 7/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:6: 100%|| 55/55 [01:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP6, train: avg_loss=2.7702351223338733, total_acc=48.160919540229884\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:6: 100%|| 14/14 [00:06<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP6, test: avg_loss=2.918459517615182, total_acc=51.60550458715596\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_7.pt\n",
            "Epoch 8/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:7: 100%|| 55/55 [01:05<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP7, train: avg_loss=2.6796715172854335, total_acc=53.333333333333336\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:7: 100%|| 14/14 [00:06<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP7, test: avg_loss=2.48905086517334, total_acc=48.85321100917431\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_8.pt\n",
            "Epoch 9/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:8: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP8, train: avg_loss=2.5668770616704766, total_acc=50.86206896551724\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:8: 100%|| 14/14 [00:06<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP8, test: avg_loss=2.7404660752841403, total_acc=51.60550458715596\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_9.pt\n",
            "Epoch 10/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:9: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP9, train: avg_loss=2.6264140367507935, total_acc=49.94252873563219\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:9: 100%|| 14/14 [00:06<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP9, test: avg_loss=2.5108004212379456, total_acc=49.31192660550459\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_10.pt\n",
            "Epoch 11/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:10: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP10, train: avg_loss=2.592329257184809, total_acc=48.04597701149425\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:10: 100%|| 14/14 [00:06<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP10, test: avg_loss=2.56490319115775, total_acc=46.330275229357795\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_11.pt\n",
            "Epoch 12/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:11: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP11, train: avg_loss=2.6177465850656683, total_acc=51.89655172413793\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:11: 100%|| 14/14 [00:06<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP11, test: avg_loss=2.4651074154036388, total_acc=47.018348623853214\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_12.pt\n",
            "Epoch 13/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:12: 100%|| 55/55 [01:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP12, train: avg_loss=2.6072197372263126, total_acc=50.3448275862069\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:12: 100%|| 14/14 [00:06<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP12, test: avg_loss=2.487783508641379, total_acc=52.293577981651374\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_13.pt\n",
            "Epoch 14/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:13: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP13, train: avg_loss=3.0318803136998955, total_acc=52.58620689655172\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:13: 100%|| 14/14 [00:06<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP13, test: avg_loss=3.1116199663707187, total_acc=49.54128440366973\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_14.pt\n",
            "Epoch 15/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:14: 100%|| 55/55 [01:05<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP14, train: avg_loss=2.9767525326121937, total_acc=50.229885057471265\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:14: 100%|| 14/14 [00:06<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP14, test: avg_loss=3.028103862489973, total_acc=53.669724770642205\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_15.pt\n",
            "Epoch 16/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:15: 100%|| 55/55 [01:05<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP15, train: avg_loss=3.0352279099551116, total_acc=48.5632183908046\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:15: 100%|| 14/14 [00:06<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP15, test: avg_loss=2.944752642086574, total_acc=46.330275229357795\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_16.pt\n",
            "Epoch 17/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:16: 100%|| 55/55 [01:06<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP16, train: avg_loss=2.943457026915117, total_acc=51.03448275862069\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:16: 100%|| 14/14 [00:06<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP16, test: avg_loss=2.938149298940386, total_acc=45.642201834862384\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_17.pt\n",
            "Epoch 18/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:17: 100%|| 55/55 [01:05<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP17, train: avg_loss=2.9085024096749046, total_acc=48.04597701149425\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:17: 100%|| 14/14 [00:06<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP17, test: avg_loss=2.978936570031302, total_acc=48.85321100917431\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_18.pt\n",
            "Epoch 19/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:18: 100%|| 55/55 [01:05<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP18, train: avg_loss=3.0207162727009167, total_acc=49.02298850574713\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:18: 100%|| 14/14 [00:06<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP18, test: avg_loss=2.849725638117109, total_acc=50.45871559633027\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_19.pt\n",
            "Epoch 20/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:19: 100%|| 55/55 [01:05<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP19, train: avg_loss=2.943289635398171, total_acc=50.632183908045974\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:19: 100%|| 14/14 [00:06<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP19, test: avg_loss=3.094244803701128, total_acc=50.91743119266055\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_20.pt\n",
            "Epoch 21/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:20: 100%|| 55/55 [01:06<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP20, train: avg_loss=2.898504801229997, total_acc=48.793103448275865\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:20: 100%|| 14/14 [00:06<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP20, test: avg_loss=2.9679926804133823, total_acc=46.10091743119266\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_21.pt\n",
            "Epoch 22/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:21: 100%|| 55/55 [01:07<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP21, train: avg_loss=3.003139517524026, total_acc=48.67816091954023\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:21: 100%|| 14/14 [00:06<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP21, test: avg_loss=2.8486708062035695, total_acc=49.31192660550459\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_22.pt\n",
            "Epoch 23/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:22: 100%|| 55/55 [01:07<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP22, train: avg_loss=2.992994512211193, total_acc=49.42528735632184\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:22: 100%|| 14/14 [00:06<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP22, test: avg_loss=2.893943565232413, total_acc=50.0\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_23.pt\n",
            "Epoch 24/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:23: 100%|| 55/55 [01:07<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP23, train: avg_loss=2.9158073035153476, total_acc=49.59770114942529\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:23: 100%|| 14/14 [00:06<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP23, test: avg_loss=3.053294931139265, total_acc=54.58715596330275\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_24.pt\n",
            "Epoch 25/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:24: 100%|| 55/55 [01:07<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP24, train: avg_loss=nan, total_acc=49.71264367816092\n",
            "Evaluating on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_test:24: 100%|| 14/14 [00:06<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EP24, test: avg_loss=2.9461527381624495, total_acc=44.95412844036697\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_25.pt\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The training records for BERT-Base model**"
      ],
      "metadata": {
        "id": "fnL7r9cYVg6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJfXi5p-CpdQ",
        "outputId": "3de0f0be-127a-41db-bb62-4d2340137ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing dataset...\n",
            "Dataset loaded: 2176 samples.\n",
            "Initializing model...\n",
            "Total Parameters: 14273345\n",
            "Starting training...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:0: 100%|| 68/68 [00:04<00:00, 14.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP0, train: avg_loss=4.75022161357543, total_acc=49.26470588235294\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_1.pt\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:1: 100%|| 68/68 [00:04<00:00, 16.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP1, train: avg_loss=3.8224641259978798, total_acc=51.0110294117647\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_2.pt\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:2: 100%|| 68/68 [00:04<00:00, 16.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP2, train: avg_loss=3.1147766674266144, total_acc=48.713235294117645\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_3.pt\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:3: 100%|| 68/68 [00:05<00:00, 12.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP3, train: avg_loss=2.867560814408695, total_acc=49.4485294117647\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_4.pt\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:4: 100%|| 68/68 [00:04<00:00, 13.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP4, train: avg_loss=2.6595516695695767, total_acc=47.748161764705884\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_5.pt\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:5: 100%|| 68/68 [00:04<00:00, 15.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP5, train: avg_loss=2.591632083934896, total_acc=51.424632352941174\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_6.pt\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:6: 100%|| 68/68 [00:04<00:00, 14.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP6, train: avg_loss=2.5894522316315594, total_acc=51.88419117647059\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_7.pt\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:7: 100%|| 68/68 [00:04<00:00, 15.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP7, train: avg_loss=2.408834546804428, total_acc=50.32169117647059\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_8.pt\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:8: 100%|| 68/68 [00:04<00:00, 15.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP8, train: avg_loss=2.3865403515451096, total_acc=48.943014705882355\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_9.pt\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:9: 100%|| 68/68 [00:04<00:00, 14.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP9, train: avg_loss=2.478575473322588, total_acc=50.275735294117645\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_10.pt\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:10: 100%|| 68/68 [00:04<00:00, 15.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP10, train: avg_loss=2.398171454668045, total_acc=48.2077205882353\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_11.pt\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:11: 100%|| 68/68 [00:04<00:00, 15.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP11, train: avg_loss=2.320494572905933, total_acc=49.54044117647059\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_12.pt\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:12: 100%|| 68/68 [00:05<00:00, 13.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP12, train: avg_loss=2.393186472794589, total_acc=49.67830882352941\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_13.pt\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:13: 100%|| 68/68 [00:04<00:00, 16.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP13, train: avg_loss=2.311805404284421, total_acc=48.713235294117645\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_14.pt\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:14: 100%|| 68/68 [00:04<00:00, 15.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP14, train: avg_loss=2.3287406441043403, total_acc=49.356617647058826\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_15.pt\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:15: 100%|| 68/68 [00:04<00:00, 13.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP15, train: avg_loss=2.387318514725741, total_acc=50.82720588235294\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_16.pt\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:16: 100%|| 68/68 [00:04<00:00, 16.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP16, train: avg_loss=2.399014848120072, total_acc=49.17279411764706\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_17.pt\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:17: 100%|| 68/68 [00:04<00:00, 15.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP17, train: avg_loss=2.3506765768808475, total_acc=50.18382352941177\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_18.pt\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:18: 100%|| 68/68 [00:04<00:00, 13.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP18, train: avg_loss=2.3088366634705486, total_acc=50.78125\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_19.pt\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EP_train:19: 100%|| 68/68 [00:04<00:00, 15.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP19, train: avg_loss=2.3829231490107143, total_acc=50.82720588235294\n",
            "Checkpoint saved: /content/drive/MyDrive/final_project/data/bert_checkpoints/bert_checkpoint_epoch_20.pt\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "try:\n",
        "    BASE_DIR = Path(__file__).resolve().parent\n",
        "except NameError:\n",
        "    # For interactive environments like Jupyter\n",
        "    BASE_DIR = Path(os.getcwd())\n",
        "\n",
        "CHECKPOINT_DIR = BASE_DIR.joinpath('data/bert_checkpoints')\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.utcnow().timestamp()\n",
        "LOG_DIR = BASE_DIR.joinpath(f'data/logs/bert_experiment_{timestamp}')\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MAX_LEN = 64\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 0.01\n",
        "BETAS = (0.9, 0.999)\n",
        "WARMUP_STEPS = 10000\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Preparing dataset...\")\n",
        "\n",
        "    train_data = BERTDataset(df, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "    test_data = BERTDataset(df, seq_len=MAX_LEN, tokenizer=tokenizer)\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset loaded: {len(train_data)} samples.\")\n",
        "\n",
        "    print(\"Initializing model...\")\n",
        "    bert_model = BERT(\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        d_model=768,\n",
        "        n_layers=2,\n",
        "        heads=12,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    # Wrap the BERT model with language modeling components\n",
        "    bert_lm = BERTLM(bert_model, len(tokenizer.vocab))\n",
        "    bert_lm.to(device)\n",
        "\n",
        "    # Initialize the trainer\n",
        "    bert_trainer = BERTTrainer(\n",
        "        model=bert_lm,\n",
        "        train_dataloader=train_loader,\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        betas=BETAS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        log_freq=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "        bert_trainer.train(epoch)\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        checkpoint_path = CHECKPOINT_DIR.joinpath(f\"bert_checkpoint_epoch_{epoch + 1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': bert_lm.state_dict(),\n",
        "            'optimizer_state_dict': bert_trainer.optim.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "ZG2yOcUPR_S1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}